# PhÃ¢n TÃ­ch Streaming Architecture cho Realtime MCP

## 1. TÃ¬nh Tráº¡ng Hiá»‡n Táº¡i

### 1.1 NANO-IO Communication Flow
**NANO Layer** (`src/nano/core/nano/nano.c`):
```
nano_process_message() â†’
  io_push_request(json_rpc) â†’
  [Polling loop 60s timeout] â†’
  io_pop_response() â†’
  return MCP response
```

**IO Layer** (`src/io/core/io/io_worker.c`):
```
io_worker_thread() â†’
  queue_pop(request_queue) â†’
  io_process_request() â†’
  rkllm_proxy_execute() â†’
  queue_push(response_queue) â†’
  [Complete response]
```

### 1.2 RKLLM Callback Location
**Callback Ä‘Æ°á»£c Ä‘Äƒng kÃ½ táº¡i IO Layer:**
- File: `src/io/mapping/rkllm_proxy/rkllm_operations.c:121`
- Function: `rkllm_init(&llm_handle, &param, rkllm_proxy_global_callback)`
- Callback thá»±c thi: `rkllm_proxy_global_callback()` trong `rkllm_proxy.c:167`

**Callback Flow:**
```
RKLLM NPU â†’ rkllm_proxy_global_callback() â†’ callback_context buffer â†’ Complete text â†’ IO response
```

### 1.3 Queue Usage Pattern
**Thá»±c táº¿ sá»­ dá»¥ng queue:**
- NANO push JSON-RPC request vÃ o `g_io_context.request_queue`
- IO Worker pop tá»« `request_queue`, xá»­ lÃ½, push vÃ o `response_queue`
- NANO poll `response_queue` Ä‘á»ƒ láº¥y complete response
- **Káº¿t luáº­n: Queue hiá»‡n táº¡i chá»‰ há»— trá»£ complete response, khÃ´ng pháº£i streaming**

## 2. Streaming Requirements Analysis

### 2.1 Real-time MCP Streaming Spec
Theo `tmp/issues/realtime-mcp.md`:
- **Chunk-based streaming**: Cáº§n gá»­i tá»«ng chunk text khi RKLLM callback sinh ra
- **Non-blocking communication**: NANO khÃ´ng Ä‘Æ°á»£c block chá» complete response
- **Real-time response**: Client nháº­n chunk ngay láº­p tá»©c

### 2.2 Architecture Gaps
**Váº¥n Ä‘á» hiá»‡n táº¡i:**
1. **Synchronous polling**: NANO block 60s chá» complete response
2. **Buffer accumulation**: Callback chá»‰ append vÃ o buffer, khÃ´ng stream
3. **Single response model**: Queue chá»‰ há»— trá»£ 1 request = 1 response

**Cáº§n thiáº¿t:**
1. **Asynchronous callback**: Callback pháº£i notify NANO ngay khi cÃ³ chunk
2. **Multi-response model**: 1 request = N streaming chunks + 1 final response
3. **Non-blocking NANO**: NANO return immediately, client poll/websocket cho chunks

## 3. Streaming Architecture Design

### 3.1 Modified Queue System
**Current:**
```
request_queue: [request_item]
response_queue: [complete_response]
```

**Proposed:**
```
request_queue: [request_item]
response_queue: [chunk1, chunk2, ..., final_response]
stream_contexts: {request_id: {state, chunks_sent, ...}}
```

### 3.2 Callback Modification
**Current Callback:**
```c
int rkllm_proxy_global_callback(RKLLMResult* result, void* userdata, LLMCallState state) {
    // Append to buffer only
    if (result->text) {
        strcat(context->output_buffer + context->current_pos, result->text);
    }
    return 0;
}
```

**Streaming Callback:**
```c
int rkllm_streaming_callback(RKLLMResult* result, void* userdata, LLMCallState state) {
    streaming_context_t* ctx = (streaming_context_t*)userdata;
    
    if (result->text && strlen(result->text) > 0) {
        // Create streaming chunk response
        json_object *chunk_response = create_streaming_chunk(ctx->request_id, result->text, false);
        
        // Push chunk to response queue immediately
        queue_item_t chunk_item = {
            .request_id = ctx->request_id,
            .handle_id = ctx->handle_id,
            .method = "stream_chunk",
            .params = json_object_to_json_string(chunk_response),
            .timestamp = time(nullptr)
        };
        queue_push(&g_io_context.response_queue, &chunk_item);
        
        json_object_put(chunk_response);
    }
    
    if (state == RKLLM_RUN_FINISH || state == RKLLM_RUN_ERROR) {
        // Create final response
        json_object *final_response = create_streaming_final(ctx->request_id, state);
        
        queue_item_t final_item = {
            .request_id = ctx->request_id,
            .handle_id = ctx->handle_id,
            .method = "stream_final",
            .params = json_object_to_json_string(final_response),
            .timestamp = time(nullptr)
        };
        queue_push(&g_io_context.response_queue, &final_item);
        
        json_object_put(final_response);
        ctx->finished = true;
    }
    
    return 0;
}
```

### 3.3 NANO Layer Streaming
**Current NANO:**
```c
int nano_process_message() {
    io_push_request();
    
    // Block and poll
    while (timeout < 60) {
        if (io_pop_response()) break;
        sleep(1);
    }
    
    return response;
}
```

**Streaming NANO:**
```c
int nano_process_streaming_message() {
    // Push request vá»›i streaming flag
    io_push_streaming_request();
    
    // Return immediately vá»›i streaming ID
    return create_streaming_response(request_id);
}

// Separate polling function for chunks
int nano_poll_streaming_chunks(uint32_t request_id, chunk_callback_t callback) {
    while (!finished) {
        queue_item_t chunk;
        if (io_pop_response_by_id(request_id, &chunk) == 0) {
            if (strcmp(chunk.method, "stream_chunk") == 0) {
                callback(chunk.params); // Send to client
            } else if (strcmp(chunk.method, "stream_final") == 0) {
                callback(chunk.params);
                break;
            }
        }
        usleep(1000); // 1ms polling
    }
    return 0;
}
```

## 4. Implementation Strategy

### 4.1 Phase 1: Queue System Enhancement
1. **Extend queue_item_t**: Add `stream_id`, `chunk_sequence`, `is_final` fields
2. **Add streaming context**: `streaming_context_t` to track active streams
3. **Modify queue operations**: Support filtering by request_id and stream type

### 4.2 Phase 2: RKLLM Callback Streaming
1. **Create streaming callback**: Replace `rkllm_proxy_global_callback`
2. **Chunk generation**: Immediate push to response queue per callback
3. **Context management**: Track streaming state per request

### 4.3 Phase 3: NANO Streaming Interface
1. **Non-blocking request**: `nano_process_streaming_message()`
2. **Chunk polling**: `nano_poll_streaming_chunks()`
3. **WebSocket integration**: Real-time chunk delivery to client

### 4.4 Phase 4: Transport Layer Integration
1. **WebSocket streaming**: Direct chunk forwarding
2. **HTTP chunked transfer**: For HTTP clients
3. **TCP streaming**: Raw chunk streaming for TCP clients

## 5. Performance Considerations

### 5.1 Memory Management
- **Chunk buffer**: Small fixed-size buffers per chunk (256-512 bytes)
- **Context cleanup**: Auto-cleanup finished streaming contexts
- **Queue capacity**: Increase queue size for multiple concurrent streams

### 5.2 Latency Optimization
- **Immediate callback**: Zero buffering in callback
- **Micro-polling**: 1ms polling interval for real-time
- **Direct queue access**: Bypass worker thread for streaming responses

## 6. Compatibility Assessment

### 6.1 Current System Compatibility
**âœ… CÃ³ thá»ƒ tÆ°Æ¡ng thÃ­ch:**
- Queue infrastructure Ä‘Ã£ cÃ³ sáºµn
- JSON-RPC format cÃ³ thá»ƒ extend cho streaming
- Worker thread model cÃ³ thá»ƒ adapt

**âŒ Cáº§n thay Ä‘á»•i:**
- Callback accumulation logic
- NANO synchronous polling
- Single response assumption

### 6.2 Backward Compatibility
**Giáº£i phÃ¡p:**
- Keep existing `rkllm_op_run()` for sync operations
- Add new `rkllm_op_run_streaming()` for async streaming
- Client specify streaming mode in request

## 7. Complete RKLLM API Exposure

### 7.1 Client Access to All RKLLM Functions
**âœ… ÄÃšNG**: Client cÃ³ thá»ƒ gá»i **Táº¤T Cáº¢** functions cá»§a RKLLM library thÃ´ng qua Nano!

**Available RKLLM Operations cho Client:**

**Functions KHÃ”NG cáº§n handle_id (utility functions):**
```json
// Create default parameters - khÃ´ng cáº§n handle
{
  "jsonrpc": "2.0", 
  "id": 1,
  "method": "create_default_param",  // rkllm_createDefaultParam()
  "params": {}
}
// Response: {"jsonrpc": "2.0", "id": 1, "result": {"default_params": {...}}}
```

**Functions Cáº¦N handle_id (model operations):**
```json
// Step 1: Initialize model handle - tráº£ vá» handle_id
{
  "jsonrpc": "2.0", 
  "id": 2,
  "method": "init",           // rkllm_init(LLMHandle* handle, ...)
  "params": {"model_path": "models/qwen3/model.rkllm"}
}
// Response: {"jsonrpc": "2.0", "id": 2, "result": {"handle_id": 123}}

// Step 2: Táº¥t cáº£ operations khÃ¡c cáº§n handle_id
{
  "jsonrpc": "2.0", 
  "id": 3,
  "method": "run",            // rkllm_run(LLMHandle handle, ...)
  "params": {
    "handle_id": 123,
    "prompt": "Hello, how are you?"
  }
}

{
  "jsonrpc": "2.0", 
  "id": 4,
  "method": "run_async",      // rkllm_run_async(LLMHandle handle, ...)
  "params": {
    "handle_id": 123,
    "prompt": "Tell me a story"
  }
}

{
  "jsonrpc": "2.0",
  "id": 5,
  "method": "load_lora",      // rkllm_load_lora(LLMHandle handle, ...)
  "params": {
    "handle_id": 123,
    "lora_adapter_path": "models/lora/lora.rkllm"
  }
}

{
  "jsonrpc": "2.0",
  "id": 6,
  "method": "set_chat_template",  // rkllm_set_chat_template(LLMHandle handle, ...)
  "params": {
    "handle_id": 123,
    "system_prompt": "You are a helpful assistant",
    "prompt_prefix": "User: ",
    "prompt_postfix": "\nAssistant: "
  }
}

{
  "jsonrpc": "2.0",
  "id": 7,
  "method": "set_function_tools", // rkllm_set_function_tools(LLMHandle handle, ...)
  "params": {
    "handle_id": 123,
    "system_prompt": "You can call functions",
    "tools": "[{\"name\": \"get_weather\", \"parameters\": {...}}]"
  }
}

{
  "jsonrpc": "2.0",
  "id": 8,
  "method": "clear_kv_cache",     // rkllm_clear_kv_cache(LLMHandle handle, ...)
  "params": {
    "handle_id": 123,
    "keep_system_prompt": true
  }
}

{
  "jsonrpc": "2.0", 
  "id": 9,
  "method": "abort",              // rkllm_abort(LLMHandle handle)
  "params": {"handle_id": 123}
}

{
  "jsonrpc": "2.0",
  "id": 10,
  "method": "is_running",         // rkllm_is_running(LLMHandle handle)
  "params": {"handle_id": 123}
}

// Step 3: Cleanup when done
{
  "jsonrpc": "2.0",
  "id": 11, 
  "method": "destroy",            // rkllm_destroy(LLMHandle handle)
  "params": {"handle_id": 123}
}
```

### 7.2 Complete API Mapping vá»›i Handle Translation
**NANO = Complete RKLLM Proxy vá»›i Handle Management:**
- **15 RKLLM functions** â†’ **15 JSON-RPC methods**
- **Handle translation**: NANO `handle_id` â†’ IO `LLMHandle` (actual pointer)
- **Parameter mapping**: JSON params â†’ RKLLM structs
- **Result formatting**: RKLLM results â†’ JSON responses

**Handle Translation Flow:**
```
Client request: {"method": "run", "params": {"handle_id": 123, "prompt": "hello"}}
     â†“
NANO: Validates request, forwards to IO vá»›i handle_id
     â†“  
IO Worker: handle_id â†’ LLMHandle (via handle_pool_get())
     â†“
RKLLM: rkllm_run(LLMHandle, RKLLMInput*, RKLLMInferParam*, userdata)
     â†“
IO Worker: RKLLM result â†’ JSON response  
     â†“
NANO: Forward JSON response to client
```

**Functions BY Category:**
1. **Utility (no handle needed):**
   - `create_default_param` â†’ `rkllm_createDefaultParam()`

2. **Handle lifecycle:**
   - `init` â†’ `rkllm_init(LLMHandle* handle, ...)` - creates handle
   - `destroy` â†’ `rkllm_destroy(LLMHandle handle)` - destroys handle

3. **Model operations (need handle):**
   - `run`, `run_async`, `abort`, `is_running`
   - `load_lora`, `load_prompt_cache`, `release_prompt_cache`  
   - `clear_kv_cache`, `get_kv_cache_size`
   - `set_chat_template`, `set_function_tools`, `set_cross_attn_params`

**Handle Pool Management:**
- **Current**: `MAX_HANDLES = 1` (RK3588 limitation)
- **Pool operations**: `handle_pool_get()`, `handle_pool_set_handle()`, `handle_pool_destroy()`
- **Future**: Increase pool size khi hardware cho phÃ©p multiple models

### 7.3 Architecture Benefits
**1. Universal RKLLM Client with Handle Management:**
```
Client â†’ JSON-RPC â†’ NANO â†’ IO Worker â†’ Handle Pool[1] â†’ RKLLM â†’ RK3588 NPU
      â†           â†      â†            â†               â†        â†
```

**Current Limitations (Hardware):**
- **Single model**: Only 1 handle active (RK3588 memory constraint)
- **Single worker**: Only 1 IO worker thread (NPU limitation)
- **Serial processing**: Requests queued and processed sequentially

**Future Scalability (When Hardware Improves):**
- **Multiple models**: `MAX_HANDLES = 8` â†’ 8 concurrent model instances
- **Multiple workers**: `WORKER_COUNT = 4` â†’ 4 parallel inference threads
- **Parallel processing**: Multiple clients can use different models simultaneously

**2. Language Agnostic vá»›i Handle Translation:**
```python
# Python client - proper handle lifecycle
# Get default params (no handle needed)
defaults = nano.call("create_default_param", {})

# Initialize model (returns handle_id)  
result = nano.call("init", {"model_path": "qwen3.rkllm"})
handle_id = result["result"]["handle_id"]

# Model operations (need handle_id)
nano.call("set_function_tools", {
    "handle_id": handle_id, 
    "system_prompt": "You can call functions",
    "tools": "[...]"
})

# Inference (need handle_id)
result = nano.call("run", {
    "handle_id": handle_id, 
    "prompt": "Hello"
})

# Cleanup (need handle_id)  
nano.call("destroy", {"handle_id": handle_id})
```

```javascript
// JavaScript client - handle management
// Utility function (no handle)
const defaults = await nano.call("create_default_param", {});

// Initialize (returns handle_id)
const {handle_id} = await nano.call("init", {model_path: "qwen3.rkllm"});

// Model operations (need handle_id)
await nano.call("load_lora", {handle_id, lora_adapter_path: "creative.rkllm"});
await nano.call("set_chat_template", {
    handle_id, 
    system_prompt: "You are creative assistant"
});

// Inference (need handle_id)  
const result = await nano.call("run_async", {handle_id, prompt: "Story"});

// Cleanup (need handle_id)
await nano.call("destroy", {handle_id});
```

**3. Network Accessible NPU with Resource Management:**
- **Handle sharing**: Multiple clients can request same handle_id (managed by pool)
- **Resource cleanup**: Automatic handle destruction on client disconnect
- **Load balancing**: Future support for multiple Nano instances with handle distribution

## 8. Káº¿t Luáº­n

### 8.1 CÃ¢u Tráº£ Lá»i Cho CÃ¡c CÃ¢u Há»i
1. **"NANO Ä‘ang thá»±c sá»± vÃ o queue Ä‘áº§u ra cá»§a IO Ä‘á»ƒ láº¥y dá»¯ liá»‡u, hay chá»‰ lÃ  lÃ½ thuyáº¿t?"**
   - âœ… **THá»°C Sá»°**: NANO call `io_pop_response()` Ä‘á»ƒ láº¥y tá»« `g_io_context.response_queue`

2. **"HÃ m callback truyá»n vÃ o cho rkllm Ä‘á»ƒ nháº­n Ä‘áº§u ra cá»§a rkllm Ä‘ang á»Ÿ IO hay á»Ÿ NANO?"**
   - âœ… **á»ž IO**: Callback `rkllm_proxy_global_callback()` registered trong `rkllm_operations.c`

3. **"Kiáº¿n trÃºc hiá»‡n táº¡i cá»§a chÃºng ta Ä‘á»§ Ä‘á»ƒ cho IO truyá»n stream vá» cho NANO chÆ°a?"**
   - âŒ **CHÆ¯A Äá»¦**: Cáº§n modify callback Ä‘á»ƒ push chunks thay vÃ¬ accumulate buffer
   - âŒ **CHÆ¯A Äá»¦**: Cáº§n extend queue system cho multi-response per request
   - âŒ **CHÆ¯A Äá»¦**: Cáº§n non-blocking NANO interface

4. **"Client cÃ³ thá»ƒ gá»i báº¥t ká»³ hÃ m nÃ o mÃ  RKLLM cÃ³ khÃ´ng?"**
   - âœ… **HOÃ€N TOÃ€N ÄÃšNG**: Client cÃ³ access Ä‘áº¿n **15/15 RKLLM functions**
   - ðŸ”§ **Handle translation**: NANO chuyá»ƒn Ä‘á»•i `handle_id` â†’ `LLMHandle` cho IO
   - ï¿½ **1 function khÃ´ng cáº§n handle**: `create_default_param`
   - ðŸ”’ **14 functions cáº§n handle**: Táº¥t cáº£ model operations
   - ðŸš€ **Future**: Sáº½ support multiple handles khi hardware cho phÃ©p

### 8.2 Current Architecture Constraints
**Hardware Limitations (RK3588):**
- **Single model memory**: NPU chá»‰ load Ä‘Æ°á»£c 1 model cÃ¹ng lÃºc (~932MB Qwen3)
- **Memory bandwidth**: KhÃ´ng Ä‘á»§ Ä‘á»ƒ run multiple models parallel
- **Worker limitation**: 1 IO worker thread Ä‘á»ƒ avoid NPU conflicts

**Design cho Future Scalability:**
```c
// Current settings (constrained by hardware)
#define MAX_HANDLES 1        // Will increase to 8+ later
#define WORKER_COUNT 1       // Will increase to 4+ later
#define MAX_QUEUE_SIZE 100   // Already ready for high throughput

// Future settings (when hardware improves)
#define MAX_HANDLES 8        // Multiple model instances
#define WORKER_COUNT 4       // Parallel processing
#define MAX_QUEUE_SIZE 1000  // High concurrency support
```

**Client Handle Management Flow:**
```
Client A: init(qwen3) â†’ handle_id=123 â†’ run(123, "hello") â†’ destroy(123)
Client B: [waits] â†’ init(gemma) â†’ handle_id=124 â†’ run(124, "story") â†’ destroy(124)
// Currently sequential due to hardware, future will be parallel
```

### 8.3 Action Items
1. **Immediate**: Implement streaming callback prototype (with handle_id tracking)
2. **Short-term**: Extend queue system for multi-response per handle
3. **Medium-term**: Create NANO streaming interface with handle context
4. **Long-term**: Full WebSocket streaming integration
5. **Future**: Scale handle pool and worker count when hardware allows

### 8.4 Effort Estimation
- **Queue modification**: 2-3 days (add handle_id filtering)
- **Streaming callback**: 1-2 days (maintain handle context)
- **NANO streaming**: 3-4 days (handle-aware streaming)
- **Transport integration**: 5-7 days (handle lifecycle management)
- **Total**: ~2 weeks for complete streaming implementation

**Note**: All streaming implementation will preserve handle management architecture for future scalability.
