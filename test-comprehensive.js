#!/usr/bin/env node

/**
 * COMPREHENSIVE RKLLM TEST SUITE - All Functions + Server Hardening
 * Tests complete API coverage, error handling, and crash resilience
 */

const ServerManager = require('./tests/lib/server-manager');
const TestClient = require('./tests/lib/test-client');
const { createRKLLMParam, createRKLLMInput, createRKLLMInferParam, TEST_MODELS } = require('./tests/lib/test-helpers');
const fs = require('fs');
const path = require('path');

async function runComprehensiveTests() {
  console.log('ðŸš€ COMPREHENSIVE RKLLM TEST SUITE - ALL FUNCTIONS + HARDENING');
  console.log('='.repeat(80));
  console.log('ðŸŽ¯ Testing complete API coverage, error handling, and resilience');
  console.log('');

  const serverManager = new ServerManager();
  const client = new TestClient();
  
  let testNumber = 0;
  const results = {
    passed: 0,
    failed: 0,
    errors: []
  };

  try {
    // Setup
    console.log('ðŸ”§ SETUP');
    console.log('='.repeat(30));
    await serverManager.startServer();
    await client.connect();
    console.log('âœ… Server ready for comprehensive testing\n');

    // ===== CORE API TESTS =====
    await runTestSection('CORE API FUNCTIONS', [
      {
        name: 'createDefaultParam',
        test: async () => {
          const result = await client.sendRequest('rkllm.createDefaultParam', []);
          if (!result.result || typeof result.result.max_context_len !== 'number') {
            throw new Error('createDefaultParam failed - invalid structure');
          }
          console.log(`      ðŸ“Š Default context length: ${result.result.max_context_len}`);
        }
      },
      {
        name: 'get_constants',
        test: async () => {
          const result = await client.sendRequest('rkllm.get_constants', []);
          if (!result.result || !result.result.LLM_CALL_STATES || !result.result.INPUT_TYPES) {
            throw new Error('get_constants failed - missing required enums');
          }
          console.log(`      ðŸ“Š Constants loaded: ${Object.keys(result.result).length} enums`);
        }
      }
    ]);

    // ===== MODEL LIFECYCLE TESTS =====
    await runTestSection('MODEL LIFECYCLE', [
      {
        name: 'Model Initialization (Sync)',
        test: async () => {
          const syncParams = createRKLLMParam(TEST_MODELS.NORMAL);
          syncParams.is_async = false;
          syncParams.max_new_tokens = 10;
          
          const result = await client.sendRequest('rkllm.init', [null, syncParams, null]);
          if (!result.result || !result.result.success) {
            throw new Error('Sync model initialization failed');
          }
          await new Promise(resolve => setTimeout(resolve, 2000));
          console.log(`      âœ… Sync model initialized successfully`);
        }
      },
      {
        name: 'Model State Queries',
        test: async () => {
          const isRunning = await client.sendRequest('rkllm.is_running', [null]);
          if (isRunning.error) {
            throw new Error('is_running check failed');
          }
          console.log(`      ðŸ“Š Model running state checked`);
        }
      },
      {
        name: 'Basic Inference (Sync)',
        test: async () => {
          const input = createRKLLMInput('Hello');
          const inferParams = createRKLLMInferParam();
          
          const result = await client.sendRequest('rkllm.run', [null, input, inferParams, null]);
          if (!result.result || !result.result.text) {
            throw new Error('Sync inference failed');
          }
          console.log(`      âœ… Generated: "${result.result.text}"`);
        }
      },
      {
        name: 'Model Cleanup',
        test: async () => {
          const result = await client.sendRequest('rkllm.destroy', [null]);
          if (!result.result || !result.result.success) {
            throw new Error('Model destruction failed');
          }
          await new Promise(resolve => setTimeout(resolve, 1000));
          console.log(`      âœ… Model destroyed successfully`);
        }
      }
    ]);

    // ===== STREAMING TESTS =====
    await runTestSection('ASYNC STREAMING', [
      {
        name: 'Async Model Setup',
        test: async () => {
          const asyncParams = createRKLLMParam(TEST_MODELS.NORMAL);
          asyncParams.is_async = true;
          asyncParams.max_new_tokens = 30;
          asyncParams.temperature = 0.8;
          
          const result = await client.sendRequest('rkllm.init', [null, asyncParams, null]);
          if (!result.result || !result.result.success) {
            throw new Error('Async model initialization failed');
          }
          await new Promise(resolve => setTimeout(resolve, 3000));
          console.log(`      âœ… Async model initialized`);
        }
      },
      {
        name: 'Real-time Token Streaming',
        test: async () => {
          const input = createRKLLMInput('Write a haiku about technology.');
          const inferParams = createRKLLMInferParam();
          
          const streamingResult = await client.sendRawJsonStreamingRequest('rkllm.run', [null, input, inferParams, null]);
          
          if (!streamingResult.tokens || streamingResult.tokens.length < 3) {
            throw new Error(`Insufficient streaming tokens - got ${streamingResult.tokens.length}`);
          }
          
          console.log(`      âœ… Streamed ${streamingResult.tokens.length} tokens: "${streamingResult.fullText}"`);
        }
      },
      {
        name: 'Stream Abort Capability',
        test: async () => {
          // Start a long generation
          const input = createRKLLMInput('Write a very long story about...');
          const inferParams = createRKLLMInferParam();
          
          // Start async operation but don't wait
          const promise = client.sendRequest('rkllm.run', [null, input, inferParams, null]);
          
          // Abort after short delay
          await new Promise(resolve => setTimeout(resolve, 500));
          const abortResult = await client.sendRequest('rkllm.abort', [null]);
          
          if (abortResult.error) {
            throw new Error('Abort command failed');
          }
          console.log(`      âœ… Stream abort successful`);
        }
      }
    ]);

    // ===== ADVANCED INFERENCE MODES =====
    await runTestSection('ADVANCED INFERENCE MODES', [
      {
        name: 'Hidden States Extraction',
        test: async () => {
          const input = createRKLLMInput('Extract hidden representations.');
          const inferParams = createRKLLMInferParam();
          inferParams.mode = 1; // RKLLM_INFER_GET_LAST_HIDDEN_LAYER
          
          const result = await client.sendRequest('rkllm.run', [null, input, inferParams, null]);
          
          if (!result.result || !result.result.last_hidden_layer) {
            throw new Error('Hidden states extraction failed');
          }
          
          const hiddenLayer = result.result.last_hidden_layer;
          if (hiddenLayer.embd_size > 0 && hiddenLayer.hidden_states) {
            console.log(`      âœ… Hidden states: ${hiddenLayer.embd_size}D x ${hiddenLayer.num_tokens} tokens`);
          } else {
            console.log(`      â„¹ï¸  Hidden states: ${hiddenLayer.embd_size}D (no data - normal for some models)`);
          }
        }
      },
      {
        name: 'Logits Extraction',
        test: async () => {
          const input = createRKLLMInput('Get prediction probabilities.');
          const inferParams = createRKLLMInferParam();
          inferParams.mode = 2; // RKLLM_INFER_GET_LOGITS
          
          const result = await client.sendRequest('rkllm.run', [null, input, inferParams, null]);
          
          if (!result.result || !result.result.logits) {
            throw new Error('Logits extraction failed');
          }
          
          const logits = result.result.logits;
          if (logits.vocab_size > 0 && logits.logits) {
            console.log(`      âœ… Logits: ${logits.vocab_size} vocab size x ${logits.num_tokens} tokens`);
          } else {
            console.log(`      â„¹ï¸  Logits: ${logits.vocab_size} vocab size (no data - normal for some models)`);
          }
        }
      }
    ]);

    // ===== CACHE OPERATIONS =====
    await runTestSection('CACHE OPERATIONS', [
      {
        name: 'KV Cache Size Query',
        test: async () => {
          const result = await client.sendRequest('rkllm.get_kv_cache_size', [null]);
          
          if (result.error && result.error.message.includes('not implemented')) {
            console.log(`      â„¹ï¸  KV cache size query not implemented yet`);
            return;
          }
          
          if (result.error) {
            throw new Error(`KV cache size query failed: ${result.error.message}`);
          }
          
          console.log(`      âœ… KV cache size queried successfully`);
        }
      },
      {
        name: 'KV Cache Clear',
        test: async () => {
          const result = await client.sendRequest('rkllm.clear_kv_cache', [null, 1, null, null]);
          
          if (result.error && result.error.message.includes('not implemented')) {
            console.log(`      â„¹ï¸  KV cache clear not implemented yet`);
            return;
          }
          
          if (result.error) {
            throw new Error(`KV cache clear failed: ${result.error.message}`);
          }
          
          console.log(`      âœ… KV cache cleared successfully`);
        }
      }
    ]);

    // ===== LORA ADAPTER TESTS =====
    await runTestSection('LORA ADAPTER FUNCTIONALITY', [
      {
        name: 'LoRa Model Setup',
        test: async () => {
          // First destroy current model
          await client.sendRequest('rkllm.destroy', [null]);
          await new Promise(resolve => setTimeout(resolve, 1000));
          
          // Initialize with LoRa base model
          const loraParams = createRKLLMParam(TEST_MODELS.LORA);
          loraParams.is_async = false;
          loraParams.max_new_tokens = 10;
          
          const result = await client.sendRequest('rkllm.init', [null, loraParams, null]);
          if (!result.result || !result.result.success) {
            throw new Error('LoRa base model initialization failed');
          }
          await new Promise(resolve => setTimeout(resolve, 2000));
          console.log(`      âœ… LoRa base model initialized`);
        }
      },
      {
        name: 'LoRa Adapter Loading',
        test: async () => {
          const loraAdapter = {
            lora_adapter_path: "/home/x/Projects/nano/models/lora/lora.rkllm",
            lora_adapter_name: "test_adapter",
            scale: 1.0
          };
          
          const result = await client.sendRequest('rkllm.load_lora', [null, loraAdapter]);
          
          if (result.error && result.error.message.includes('not implemented')) {
            console.log(`      â„¹ï¸  LoRa adapter loading not implemented yet`);
            return;
          }
          
          if (result.error) {
            throw new Error(`LoRa adapter loading failed: ${result.error.message}`);
          }
          
          console.log(`      âœ… LoRa adapter loaded successfully`);
        }
      }
    ]);

    // ===== PROMPT CACHE TESTS =====
    await runTestSection('PROMPT CACHE OPERATIONS', [
      {
        name: 'Prompt Cache Loading',
        test: async () => {
          const cachePath = "/tmp/test_prompt_cache";
          const result = await client.sendRequest('rkllm.load_prompt_cache', [null, cachePath]);
          
          if (result.error && result.error.message.includes('not implemented')) {
            console.log(`      â„¹ï¸  Prompt cache loading not implemented yet`);
            return;
          }
          
          if (result.error) {
            console.log(`      âš ï¸  Prompt cache loading failed (expected if cache doesn't exist): ${result.error.message}`);
            return;
          }
          
          console.log(`      âœ… Prompt cache loaded successfully`);
        }
      },
      {
        name: 'Prompt Cache Release',
        test: async () => {
          const result = await client.sendRequest('rkllm.release_prompt_cache', [null]);
          
          if (result.error && result.error.message.includes('not implemented')) {
            console.log(`      â„¹ï¸  Prompt cache release not implemented yet`);
            return;
          }
          
          if (result.error) {
            throw new Error(`Prompt cache release failed: ${result.error.message}`);
          }
          
          console.log(`      âœ… Prompt cache released successfully`);
        }
      }
    ]);

    // ===== TEMPLATE & TOOLS TESTS =====
    await runTestSection('TEMPLATES & FUNCTION TOOLS', [
      {
        name: 'Chat Template Configuration',
        test: async () => {
          const systemPrompt = "You are a helpful assistant.";
          const promptPrefix = "<|user|>";
          const promptPostfix = "<|assistant|>";
          
          const result = await client.sendRequest('rkllm.set_chat_template', [null, systemPrompt, promptPrefix, promptPostfix]);
          
          if (result.error && result.error.message.includes('not implemented')) {
            console.log(`      â„¹ï¸  Chat template configuration not implemented yet`);
            return;
          }
          
          if (result.error) {
            throw new Error(`Chat template configuration failed: ${result.error.message}`);
          }
          
          console.log(`      âœ… Chat template configured successfully`);
        }
      },
      {
        name: 'Function Tools Setup',
        test: async () => {
          const systemPrompt = "You can use the following tools:";
          const tools = JSON.stringify([{
            name: "calculate",
            description: "Perform calculations",
            parameters: { type: "number" }
          }]);
          const toolResponse = "Tool executed successfully";
          
          const result = await client.sendRequest('rkllm.set_function_tools', [null, systemPrompt, tools, toolResponse]);
          
          if (result.error && result.error.message.includes('not implemented')) {
            console.log(`      â„¹ï¸  Function tools setup not implemented yet`);
            return;
          }
          
          if (result.error) {
            throw new Error(`Function tools setup failed: ${result.error.message}`);
          }
          
          console.log(`      âœ… Function tools configured successfully`);
        }
      }
    ]);

    // ===== ERROR HANDLING & RESILIENCE TESTS =====
    await runTestSection('ERROR HANDLING & RESILIENCE', [
      {
        name: 'Malformed JSON Handling',
        test: async () => {
          try {
            // Send malformed JSON
            const malformedJson = '{"jsonrpc": "2.0", "method": "invalid", "params": [broken}';
            
            // This should not crash the server
            const response = await client.sendRawRequest(malformedJson);
            
            // Server should respond with parse error, not crash
            console.log(`      âœ… Server handled malformed JSON gracefully`);
          } catch (error) {
            // Connection error is acceptable - means server detected bad input
            if (error.message.includes('connection') || error.message.includes('timeout')) {
              console.log(`      âœ… Server protected against malformed JSON`);
            } else {
              throw error;
            }
          }
        }
      },
      {
        name: 'Invalid Method Handling',
        test: async () => {
          const result = await client.sendRequest('rkllm.nonexistent_method', []);
          
          if (!result.error) {
            throw new Error('Server should return error for invalid method');
          }
          
          if (result.error.code === -32601) { // Method not found
            console.log(`      âœ… Invalid method properly rejected with JSON-RPC error`);
          } else {
            throw new Error(`Unexpected error code: ${result.error.code}`);
          }
        }
      },
      {
        name: 'Invalid Parameters Handling',
        test: async () => {
          // Try to initialize with completely invalid parameters
          const invalidParams = { completely_invalid: "data" };
          
          const result = await client.sendRequest('rkllm.init', [null, invalidParams, null]);
          
          if (!result.error) {
            throw new Error('Server should return error for invalid parameters');
          }
          
          console.log(`      âœ… Invalid parameters properly rejected: ${result.error.message}`);
        }
      },
      {
        name: 'Model Not Loaded Handling',
        test: async () => {
          // Ensure no model is loaded
          await client.sendRequest('rkllm.destroy', [null]);
          await new Promise(resolve => setTimeout(resolve, 500));
          
          // Try to run inference without a model
          const input = createRKLLMInput('This should fail');
          const inferParams = createRKLLMInferParam();
          
          const result = await client.sendRequest('rkllm.run', [null, input, inferParams, null]);
          
          if (!result.error) {
            throw new Error('Server should return error when no model is loaded');
          }
          
          console.log(`      âœ… No model state properly handled: ${result.error.message}`);
        }
      }
    ]);

    // ===== PERFORMANCE & LOAD TESTS =====
    await runTestSection('PERFORMANCE & LOAD TESTING', [
      {
        name: 'Response Time Measurement',
        test: async () => {
          // Set up a fresh model for performance testing
          const params = createRKLLMParam(TEST_MODELS.NORMAL);
          params.is_async = false;
          params.max_new_tokens = 5;
          
          await client.sendRequest('rkllm.init', [null, params, null]);
          await new Promise(resolve => setTimeout(resolve, 2000));
          
          const input = createRKLLMInput('Quick test');
          const inferParams = createRKLLMInferParam();
          
          const startTime = Date.now();
          const result = await client.sendRequest('rkllm.run', [null, input, inferParams, null]);
          const endTime = Date.now();
          
          if (!result.result) {
            throw new Error('Performance test failed - no result');
          }
          
          const responseTime = endTime - startTime;
          console.log(`      ðŸ“Š Response time: ${responseTime}ms`);
          
          if (responseTime > 15000) {
            throw new Error(`Performance degraded - response too slow: ${responseTime}ms`);
          }
        }
      },
      {
        name: 'Memory Usage Monitoring',
        test: async () => {
          // Check server process memory usage
          const memBefore = process.memoryUsage();
          
          // Run several inference operations
          const input = createRKLLMInput('Memory test');
          const inferParams = createRKLLMInferParam();
          
          for (let i = 0; i < 5; i++) {
            await client.sendRequest('rkllm.run', [null, input, inferParams, null]);
          }
          
          const memAfter = process.memoryUsage();
          const memDiff = memAfter.heapUsed - memBefore.heapUsed;
          
          console.log(`      ðŸ“Š Memory change: ${Math.round(memDiff / 1024 / 1024)}MB`);
          
          if (memDiff > 100 * 1024 * 1024) { // 100MB threshold
            console.log(`      âš ï¸  Potential memory leak detected: ${Math.round(memDiff / 1024 / 1024)}MB increase`);
          } else {
            console.log(`      âœ… Memory usage stable`);
          }
        }
      }
    ]);

  } catch (error) {
    console.error(`\nâŒ COMPREHENSIVE TEST FAILED: ${error.message}`);
    results.errors.push(error.message);
    results.failed++;
  } finally {
    // Cleanup
    console.log('\nðŸ§¹ CLEANUP');
    try {
      await client.ensureCleanState();
      client.disconnect();
      serverManager.stopServer();
      console.log('âœ… Cleanup completed');
    } catch (error) {
      console.log(`âš ï¸  Cleanup warning: ${error.message}`);
    }
  }

  // Final results
  console.log('\n' + '='.repeat(80));
  console.log(`ðŸ“Š COMPREHENSIVE TEST RESULTS:`);
  console.log(`âœ… Passed: ${results.passed} tests`);
  console.log(`âŒ Failed: ${results.failed} tests`);
  
  if (results.errors.length > 0) {
    console.log(`\nðŸ”¥ ERRORS ENCOUNTERED:`);
    results.errors.forEach((error, i) => {
      console.log(`${i + 1}. ${error}`);
    });
  }
  
  if (results.failed === 0) {
    console.log('\nðŸŽ‰ ALL COMPREHENSIVE TESTS PASSED!');
    console.log('âœ… Server demonstrates full API coverage and resilience');
    console.log('ðŸš€ Production ready for enterprise deployment');
    process.exit(0);
  } else {
    console.log('\nðŸ›‘ SOME TESTS FAILED - SERVER NEEDS IMPROVEMENTS');
    process.exit(1);
  }

  async function runTestSection(sectionName, tests) {
    console.log(`\nðŸ“‹ ${sectionName}`);
    console.log('â”€'.repeat(sectionName.length + 4));
    
    for (const test of tests) {
      testNumber++;
      process.stdout.write(`${testNumber.toString().padStart(2)}. ${test.name}... `);
      
      try {
        await test.test();
        console.log('âœ… PASS');
        results.passed++;
      } catch (error) {
        console.log('âŒ FAIL');
        console.log(`    âŒ Error: ${error.message}`);
        results.failed++;
        results.errors.push(`${test.name}: ${error.message}`);
        
        // Continue with other tests rather than failing completely
      }
    }
  }
}

// Enhanced error handlers
process.on('uncaughtException', (error) => {
  console.error('\nðŸ’¥ Uncaught Exception:', error.message);
  console.error('ðŸ›‘ This indicates a server crash or critical bug');
  process.exit(1);
});

process.on('unhandledRejection', (reason, promise) => {
  console.error('\nðŸ’¥ Unhandled Rejection:', reason);
  console.error('ðŸ›‘ This indicates improper async error handling');
  process.exit(1);
});

// Run comprehensive tests
if (require.main === module) {
  runComprehensiveTests();
}

module.exports = runComprehensiveTests;